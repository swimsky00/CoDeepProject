{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "tokenizers>=0.14,<0.19 is required for a normal functioning of this module, but found tokenizers==0.20.3.\nTry: `pip install transformers -U` or `pip install -e '.[dev]'` if you're working with git main",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 14\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrandom\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcopy\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer, AutoModel, AutoModelForSeq2SeqLM\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments, DataCollatorForSeq2Seq, pipeline\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SentenceTransformer, util\n",
      "File \u001b[1;32mc:\\Users\\datacommand\\anaconda3\\lib\\site-packages\\transformers\\__init__.py:26\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TYPE_CHECKING\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Check the dependencies satisfy the minimal versions required.\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dependency_versions_check\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     28\u001b[0m     OptionalDependencyNotAvailable,\n\u001b[0;32m     29\u001b[0m     _LazyModule,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     47\u001b[0m     logging,\n\u001b[0;32m     48\u001b[0m )\n\u001b[0;32m     51\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mget_logger(\u001b[38;5;18m__name__\u001b[39m)  \u001b[38;5;66;03m# pylint: disable=invalid-name\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\datacommand\\anaconda3\\lib\\site-packages\\transformers\\dependency_versions_check.py:57\u001b[0m\n\u001b[0;32m     54\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_accelerate_available():\n\u001b[0;32m     55\u001b[0m             \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# not required, check version only if installed\u001b[39;00m\n\u001b[1;32m---> 57\u001b[0m     \u001b[43mrequire_version_core\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdeps\u001b[49m\u001b[43m[\u001b[49m\u001b[43mpkg\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     59\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcan\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpkg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdeps\u001b[38;5;241m.\u001b[39mkeys()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, check dependency_versions_table.py\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\datacommand\\anaconda3\\lib\\site-packages\\transformers\\utils\\versions.py:117\u001b[0m, in \u001b[0;36mrequire_version_core\u001b[1;34m(requirement)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"require_version wrapper which emits a core-specific hint on failure\"\"\"\u001b[39;00m\n\u001b[0;32m    116\u001b[0m hint \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTry: `pip install transformers -U` or `pip install -e \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.[dev]\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m` if you\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mre working with git main\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 117\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequire_version\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequirement\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhint\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\datacommand\\anaconda3\\lib\\site-packages\\transformers\\utils\\versions.py:111\u001b[0m, in \u001b[0;36mrequire_version\u001b[1;34m(requirement, hint)\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m want_ver \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    110\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m op, want_ver \u001b[38;5;129;01min\u001b[39;00m wanted\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m--> 111\u001b[0m         \u001b[43m_compare_versions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgot_ver\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwant_ver\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequirement\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpkg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhint\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\datacommand\\anaconda3\\lib\\site-packages\\transformers\\utils\\versions.py:44\u001b[0m, in \u001b[0;36m_compare_versions\u001b[1;34m(op, got_ver, want_ver, requirement, pkg, hint)\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m     40\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to compare versions for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrequirement\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: need=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mwant_ver\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m found=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgot_ver\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. This is unusual. Consider\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     41\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m reinstalling \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpkg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     42\u001b[0m     )\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ops[op](version\u001b[38;5;241m.\u001b[39mparse(got_ver), version\u001b[38;5;241m.\u001b[39mparse(want_ver)):\n\u001b[1;32m---> 44\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[0;32m     45\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrequirement\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is required for a normal functioning of this module, but found \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpkg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m==\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgot_ver\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhint\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     46\u001b[0m     )\n",
      "\u001b[1;31mImportError\u001b[0m: tokenizers>=0.14,<0.19 is required for a normal functioning of this module, but found tokenizers==0.20.3.\nTry: `pip install transformers -U` or `pip install -e '.[dev]'` if you're working with git main"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn.parameter import Parameter\n",
    "from tqdm import tqdm\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForSeq2SeqLM\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pretrained Model 불러오기\n",
    "- 둘 중 하나 쓰면 됨(tokenizer&pretrained_model 세트로)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = T5Tokenizer.from_pretrained('j5ng/et5-typos-corrector')\n",
    "pretrained_model = T5ForConditionalGeneration.from_pretrained('j5ng/et5-typos-corrector')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"cosmoquester/bart-ko-mini\")\n",
    "pretrained_model = AutoModelForSeq2SeqLM.from_pretrained(\"cosmoquester/bart-ko-mini\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"강석호교수님.csv\")\n",
    "test = pd.read_csv(\"강명인교수님.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(34138, 256)"
      ]
     },
     "execution_count": 678,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train 데이터의 라벨(실제 교수님께서 하신 말씀)을 띄어쓰기 단위(단어 단위라고 가정)로 나눠서 기존 token(logit을 가질 label)에 추가\n",
    "add_tokens = [word for sentence in train['y'] for word in sentence.split()]\n",
    "tokenizer.add_tokens(add_tokens)\n",
    "\n",
    "# token이 추가되었으니(출력되어야할 차원이 커짐) embedding 사이즈 수정 \n",
    "pretrained_model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장을 단어 단위로 나누고 tokenizer -> 추후 사용에 용이하도록 dictionary 형태로 만듬\n",
    "def tokenizer_(input):\n",
    "    input_list = [input for input in input['X']]\n",
    "    label_list = [label for label in input['y']]\n",
    "\n",
    "    input_tokenizer = tokenizer(input_list, return_tensors=\"pt\", max_length=32, truncation=True, padding=\"max_length\")['input_ids']\n",
    "    label_tokenizer = tokenizer(label_list, return_tensors=\"pt\", max_length=32, truncation=True, padding=\"max_length\")['input_ids']\n",
    "\n",
    "    tokenizer_dict = {'input_tokenizer': input_tokenizer, 'label_tokenizer': label_tokenizer}\n",
    "    return tokenizer_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_1 = train.iloc[:476,:] # 사실 이거는 하면 안되긴 함 -> 학습 시간이 너무 오래걸려서 augmentation 했던거 잘라서 학습 시도해본 것\n",
    "dataset = tokenizer_(train_1) # train 데이터 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test 데이터 토큰화\n",
    "Inference_dataset = tokenizer_(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KoBART(nn.Module):\n",
    "    def __init__(self, pretrained_model):\n",
    "        super(KoBART, self).__init__()\n",
    "\n",
    "        self.pretrained_model = pretrained_model\n",
    "        self.output_size = 34138 # token 사이즈\n",
    "        self.hidden_size = 256\n",
    "\n",
    "        # 위에 우리가 쌓고자 했던 layer\n",
    "        self.new_fc = nn.Sequential(\n",
    "            nn.Linear(self.hidden_size, self.hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.hidden_size, self.hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.hidden_size, self.hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.hidden_size, self.output_size)\n",
    "        )                  \n",
    "\n",
    "    def forward(self, input):\n",
    "        input = torch.reshape(input, (1,-1))\n",
    "        new_input = self.pretrained_model(input, output_hidden_states=True).decoder_hidden_states[-1] # pretrained model 학습 후 마지막 layer logit만 불러옴\n",
    "        new_output = self.new_fc(new_input) # 새로 쌓은 layer에 넣음음\n",
    "        return new_output\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 684,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'\n",
    "model = KoBART(pretrained_model)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.new_fc.parameters(), lr=0.01)\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 476/476 [01:24<00:00,  5.64it/s]\n",
      " 10%|█         | 1/10 [01:24<12:39, 84.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 loss: 1336.108642578125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 476/476 [01:11<00:00,  6.66it/s]\n",
      " 20%|██        | 2/10 [02:36<10:15, 76.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 loss: 1093.953369140625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 476/476 [01:35<00:00,  4.98it/s]\n",
      " 30%|███       | 3/10 [04:11<09:58, 85.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 loss: 1012.8211669921875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 476/476 [01:23<00:00,  5.68it/s]\n",
      " 40%|████      | 4/10 [05:35<08:29, 84.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 loss: 954.2654418945312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 476/476 [01:11<00:00,  6.64it/s]\n",
      " 50%|█████     | 5/10 [06:47<06:41, 80.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 loss: 913.168701171875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 476/476 [01:15<00:00,  6.27it/s]\n",
      " 60%|██████    | 6/10 [08:03<05:15, 78.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 loss: 886.44287109375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 476/476 [01:54<00:00,  4.14it/s]\n",
      " 70%|███████   | 7/10 [09:58<04:31, 90.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 loss: 861.6910400390625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 476/476 [01:48<00:00,  4.38it/s]\n",
      " 80%|████████  | 8/10 [11:47<03:12, 96.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 loss: 841.7211303710938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 476/476 [01:57<00:00,  4.04it/s]\n",
      " 90%|█████████ | 9/10 [13:45<01:43, 103.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 loss: 825.9454956054688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 476/476 [01:52<00:00,  4.23it/s]\n",
      "100%|██████████| 10/10 [15:38<00:00, 93.86s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 loss: 812.2211303710938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    temp_loss = 0\n",
    "\n",
    "    # mini batch\n",
    "    for idx in tqdm(range(len(train_1))):\n",
    "        logit = model(dataset['input_tokenizer'][idx]) # dataset['input_tokenizer'] = input train 토큰하된 것\n",
    "        pred = torch.reshape(logit, (32,34138))\n",
    "        loss = loss_fn(pred, dataset['label_tokenizer'][idx])\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        temp_loss += loss\n",
    "    \n",
    "    print(f\"{epoch} loss: {temp_loss.detach().item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 648/648 [00:27<00:00, 23.15it/s]\n"
     ]
    }
   ],
   "source": [
    "# 여기가 위에 학습한 모델로 추론한 것(근데 학습 시간이 너무 오래걸려서 끝까지 학습 안 함함)\n",
    "model.eval()\n",
    "model.to('cpu')\n",
    "\n",
    "result = []\n",
    "for sentence in tqdm(Inference_dataset['input_tokenizer']):\n",
    "    input = torch.reshape(sentence, (1,-1))\n",
    "    logit = model(input)\n",
    "    output = torch.argmax(logit[0], dim=1)\n",
    "    correction = tokenizer.decode(output, skip_special_tokens=True)\n",
    "    result.append(correction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correction_df = pd.DataFrame(result, columns=['y_hat'])\n",
    "total_df = pd.concat([correction_df, test['y']], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 688,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_hat</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>우리가</td>\n",
       "      <td>안녕하세요 데이터과학을 위한 프로그래밍 수강생 여러분</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>우리가</td>\n",
       "      <td>오늘 슈퍼바이스러닝 다섯번째 시간 시작하도록 하겠습니다</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>그래서</td>\n",
       "      <td>오늘 할 내용은 뉴럴 네트워크 이구요</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>그래서   게</td>\n",
       "      <td>원래 서포트 벡터 머신이 먼저 나와 있는데</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>그래서         수 수</td>\n",
       "      <td>순서를 바꿔서 인공신경망, 뉴로이 네트워크에 관련된 내용부터 진행하도록 하겠습니다</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>643</th>\n",
       "      <td>우리가</td>\n",
       "      <td>조금 더 빠르게 수렴할 수 수렴시킬 수 있고</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>644</th>\n",
       "      <td>우리가</td>\n",
       "      <td>그리고 성능도 좋을 수 있다 라는 일반적인 이야기가 있고요</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>645</th>\n",
       "      <td>우리가           게</td>\n",
       "      <td>그런데 이제 사실 만약에 사이킷런 말고</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>646</th>\n",
       "      <td>우리가   게</td>\n",
       "      <td>여러분이 뭔가 좀 더 텐서플로우나</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>647</th>\n",
       "      <td>그래서 우리가 수 게</td>\n",
       "      <td>파이토치 같은 거 사용해서</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>648 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     y_hat                                              y\n",
       "0    우리가                                    안녕하세요 데이터과학을 위한 프로그래밍 수강생 여러분\n",
       "1        우리가                               오늘 슈퍼바이스러닝 다섯번째 시간 시작하도록 하겠습니다\n",
       "2            그래서                                     오늘 할 내용은 뉴럴 네트워크 이구요\n",
       "3          그래서   게                                원래 서포트 벡터 머신이 먼저 나와 있는데\n",
       "4        그래서         수 수    순서를 바꿔서 인공신경망, 뉴로이 네트워크에 관련된 내용부터 진행하도록 하겠습니다\n",
       "..                     ...                                            ...\n",
       "643      우리가                                     조금 더 빠르게 수렴할 수 수렴시킬 수 있고\n",
       "644    우리가                               그리고 성능도 좋을 수 있다 라는 일반적인 이야기가 있고요\n",
       "645       우리가           게                           그런데 이제 사실 만약에 사이킷런 말고\n",
       "646           우리가   게                                  여러분이 뭔가 좀 더 텐서플로우나\n",
       "647           그래서 우리가 수 게                                  파이토치 같은 거 사용해서\n",
       "\n",
       "[648 rows x 2 columns]"
      ]
     },
     "execution_count": 688,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 648/648 [01:53<00:00,  5.69it/s]\n"
     ]
    }
   ],
   "source": [
    "# 여기는 결과가 너무 안 좋아서 그냥 pretrained 모델만 갖고(resize된 상태) decode 시킴(추가 layer을 안 쌓은 것)\n",
    "result1 = []\n",
    "for sentence in tqdm(Inference_dataset['input_tokenizer']):\n",
    "    correction = tokenizer.decode(model.pretrained_model.generate(torch.reshape(sentence,(1,-1)),max_length=50)[0], skip_special_tokens=True)\n",
    "    result1.append(correction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 655,
   "metadata": {},
   "outputs": [],
   "source": [
    "correction_df1 = pd.DataFrame(result1, columns=['y_hat'])\n",
    "total_df1 = pd.concat([correction_df1, test['y']], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 656,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_hat</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[BOS]녕하세요 [UNK]과학을 [EOS]</td>\n",
       "      <td>안녕하세요 데이터과학을 위한 프로그래밍 수강생 여러분</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[BOS] 관공서 다섯 한 시간 [EOS]</td>\n",
       "      <td>오늘 슈퍼바이스러닝 다섯번째 시간 시작하도록 하겠습니다</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[BOS] 할   뉴로이 네트워크 이구요[EOS]</td>\n",
       "      <td>오늘 할 내용은 뉴럴 네트워크 이구요</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[BOS]        [EOS]</td>\n",
       "      <td>원래 서포트 벡터 머신이 먼저 나와 있는데</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[BOS]서를 바꿔서    뉴로이   [EOS]</td>\n",
       "      <td>순서를 바꿔서 인공신경망, 뉴로이 네트워크에 관련된 내용부터 진행하도록 하겠습니다</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>643</th>\n",
       "      <td>[BOS] 더 [UNK] 수렴할 수 수 수렴시킬[EOS]</td>\n",
       "      <td>조금 더 빠르게 수렴할 수 수렴시킬 수 있고</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>644</th>\n",
       "      <td>[BOS]     [UNK]    라는   [EOS]</td>\n",
       "      <td>그리고 성능도 좋을 수 있다 라는 일반적인 이야기가 있고요</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>645</th>\n",
       "      <td>[BOS]      사이클론 말고[EOS]</td>\n",
       "      <td>그런데 이제 사실 만약에 사이킷런 말고</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>646</th>\n",
       "      <td>[BOS]이 뭔가 좀 더 텐서플로우나[EOS]</td>\n",
       "      <td>여러분이 뭔가 좀 더 텐서플로우나</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>647</th>\n",
       "      <td>[BOS]  거 [EOS]</td>\n",
       "      <td>파이토치 같은 거 사용해서</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>648 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                               y_hat  \\\n",
       "0           [BOS]녕하세요 [UNK]과학을 [EOS]   \n",
       "1            [BOS] 관공서 다섯 한 시간 [EOS]   \n",
       "2        [BOS] 할   뉴로이 네트워크 이구요[EOS]   \n",
       "3                 [BOS]        [EOS]   \n",
       "4         [BOS]서를 바꿔서    뉴로이   [EOS]   \n",
       "..                               ...   \n",
       "643  [BOS] 더 [UNK] 수렴할 수 수 수렴시킬[EOS]   \n",
       "644    [BOS]     [UNK]    라는   [EOS]   \n",
       "645          [BOS]      사이클론 말고[EOS]   \n",
       "646        [BOS]이 뭔가 좀 더 텐서플로우나[EOS]   \n",
       "647                   [BOS]  거 [EOS]   \n",
       "\n",
       "                                                 y  \n",
       "0                    안녕하세요 데이터과학을 위한 프로그래밍 수강생 여러분  \n",
       "1                   오늘 슈퍼바이스러닝 다섯번째 시간 시작하도록 하겠습니다  \n",
       "2                             오늘 할 내용은 뉴럴 네트워크 이구요  \n",
       "3                          원래 서포트 벡터 머신이 먼저 나와 있는데  \n",
       "4    순서를 바꿔서 인공신경망, 뉴로이 네트워크에 관련된 내용부터 진행하도록 하겠습니다  \n",
       "..                                             ...  \n",
       "643                       조금 더 빠르게 수렴할 수 수렴시킬 수 있고  \n",
       "644               그리고 성능도 좋을 수 있다 라는 일반적인 이야기가 있고요  \n",
       "645                          그런데 이제 사실 만약에 사이킷런 말고  \n",
       "646                             여러분이 뭔가 좀 더 텐서플로우나  \n",
       "647                                 파이토치 같은 거 사용해서  \n",
       "\n",
       "[648 rows x 2 columns]"
      ]
     },
     "execution_count": 656,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
